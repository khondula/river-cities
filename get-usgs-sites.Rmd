---
title: "get-usgs-sites"
author: "Kelly Hondula"
date: "3/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


Make a list of the census tracts

```{r}
claims_census_tracts <- claims_places_distinct$censustract %>% unique()
```

For each of the census tracts in the claims data

- check that it is 11 characters
- get first 2 characters for state fips
- read in tracts file for that state
- filter to census tract
- buffer tract and get bounding box
- look for gauges within bbox
- get info about sites and record length
- save data 

```{r}
claims_census_tract <- claims_census_tracts[3]

get_usgs_sites <- function(claims_census_tract){
  # make sure census tract is valid
  if(nchar(claims_census_tract)==11){
  state_fips <- substr(claims_census_tract, 1, 2)
  tracts_dir <- "/nfs/public-data/census-tiger-2019/TRACT"

  tract_file <- glue("{tracts_dir}/tl_2019_{state_fips}_tract.shp")
  tract_sf <- sf::st_read(tract_file) %>%
    dplyr::filter(GEOID == claims_census_tract)

  mybbox <- tract_sf %>% 
    st_transform(crs = 2163) %>%
    st_buffer(dist = 10000) %>% 
    st_transform(crs = 4326) %>%
    st_bbox() %>% 
    as.vector() %>% round(digits = 2)

  # look for usgs sites with discharge within bbox
  # use error handling to return null if no sites returned
  tract_sites <- tryCatch({
    dataRetrieval::whatNWISsites(parameterCd = "00060", 
                                             bBox = mybbox)},
    error = function(e) {return(NULL)})

  # only proceed if there are gauges within 10km
    if(!is.null(tract_sites)){
  # get info about sites
  tract_sites_info <- readNWISsite(tract_sites[["site_no"]]) %>%
    dplyr::select(site_no, state_cd, drain_area_va)
  
  # get info about record length
  site_data <- whatNWISdata(parameterCd = "00060", # discharge
                                 siteNumber = tract_sites[["site_no"]]) %>%
    arrange(begin_date) %>%
    mutate(early_enough = begin_date < "1980-01-01") %>%
    mutate(late_enough = end_date > "2000-01-01") %>%
    mutate(count_10obs = count_nu > 10) %>%
    mutate(good_record = all(early_enough, late_enough, count_10obs))
 
    # make a table with site info of interest and add city name and pop
  df <- site_data %>%
    dplyr::select(agency_cd, site_no, station_nm, dec_lat_va, dec_long_va,
                  huc_cd, data_type_cd, parm_cd, stat_cd, begin_date, end_date,
                  count_nu, early_enough, late_enough, good_record) %>%
    mutate(site_no = as.character(site_no)) %>%
    left_join(tract_sites_info) %>%
    arrange(site_no)
  
  gauges_dir <- "/nfs/khondula-data/projects/river-cities/data/gauges"
  if(!fs::dir_exists(gauges_dir)){fs::dir_create(gauges_dir)}
  if(nrow(df) > 0) {readr::write_csv(df, path = glue("{gauges_dir}/tract-{claims_census_tract}.csv"))}
  
  }}
}


```

```{r, include = FALSE, eval=FALSE}
get_usgs_sites(claims_census_tracts[3])
purrr::walk(claims_census_tracts[1:10], ~get_usgs_sites(.x))
```

Run on cluster

```{r, eval = FALSE}
pars <- data.frame(claims_census_tract = claims_census_tracts,
                   stringsAsFactors = FALSE)


sjob <- slurm_apply(get_usgs_sites, 
                    pars, 
                    jobname = 'rivercities',
                    # slurm_options = list(partition = "sesync"),
                    nodes = 4, 
                    cpus_per_node = 4,
                    submit = TRUE)

rslurm::print_job_status(sjob)
rslurm::cancel_slurm(sjob)
```

```{r, eval=FALSE}
length(list.files(gauges_dir))
```

Combine 

```{r}
# read in all csvs and save as one table
df <- fs::dir_ls("data") %>% 
  purrr::map_df(~read_csv(.x, col_types = c("cccddccccDDdllcdcd")))

df %>% readr::write_csv("river-cities.csv")

```
