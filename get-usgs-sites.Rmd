---
title: "get-usgs-sites"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dataRetrieval)
library(rosm)
library(RStoolbox)
library(sf)
library(fs)
library(leaflet)
library(ggplot2)
library(dplyr)
library(glue)
library(ggrepel)
```

Find gauge for each CBSA

```{r}
cbsa_dir <- "/nfs/public-data/census-tiger-2019/CBSA"
cbsa_file <- glue("{cbsa_dir}/tl_2019_us_cbsa.shp")
cbsa_sf <- sf::st_read(cbsa_file)
```

```{r}
my_cbsa <- cbsa_sf[["GEOID"]][380]
```

- look for gauges within bbox
- get info about sites and record length
- save data 

```{r}

get_usgs_sites <- function(my_cbsa){

  cbsa_dir <- "/nfs/public-data/census-tiger-2019/CBSA"
  cbsa_file <- glue("{cbsa_dir}/tl_2019_us_cbsa.shp")
  my_cbsa_sf <- sf::st_read(cbsa_file) %>%
    dplyr::filter(GEOID == my_cbsa)

  my_bbox <- my_cbsa_sf %>% st_bbox()
  # look for usgs sites with discharge within bbox
  # use error handling to return null if no sites returned
  usgs_sites <- tryCatch({
    dataRetrieval::whatNWISsites(parameterCd = "00060", 
                                             bBox = my_bbox)},
    error = function(e) {return(NULL)})
  # only proceed if there are gauges
  if(!is.null(usgs_sites)){
  # get info about sites
  usgs_sites_info <- readNWISsite(usgs_sites[["site_no"]]) %>%
    dplyr::select(site_no, state_cd, drain_area_va)
  # get info about record length
  site_data <- whatNWISdata(parameterCd = "00060", # discharge
                                 siteNumber = usgs_sites[["site_no"]]) %>%
    arrange(begin_date) %>%
    mutate(early_enough = begin_date < "1980-01-01") %>%
    mutate(late_enough = end_date > "2000-01-01") %>%
    mutate(count_10obs = count_nu > 10)
  
  site_data_goodrecords <- site_data %>% 
    dplyr::filter(early_enough, late_enough, count_10obs)
  
  if(nrow(site_data_goodrecords) > 0){
    
  good_sites_sf <- site_data_goodrecords %>% 
    dplyr::select(site_no, dec_lat_va, dec_long_va) %>% 
    distinct() %>%
    st_as_sf(coords = c("dec_long_va", "dec_lat_va"), crs = 4326)

  # make a table with site info of interest and add city name and pop
  df <- site_data_goodrecords %>%
    dplyr::select(agency_cd, site_no, station_nm, dec_lat_va, dec_long_va,
                  huc_cd, data_type_cd, parm_cd, stat_cd, begin_date, end_date,
                  count_nu) %>%
    mutate(site_no = as.character(site_no)) %>%
    arrange(site_no) %>%
    left_join(usgs_sites_info) %>%
    mutate(cbsa_geoid = my_cbsa_sf$GEOID,
           cbsa_name = my_cbsa_sf$NAME)
  
  gauges_dir <- "/nfs/khondula-data/projects/river-cities/data/gauges-cbsa"
  if(!fs::dir_exists(gauges_dir)){fs::dir_create(gauges_dir)}
  if(nrow(df) > 0) {readr::write_csv(df, path = glue("{gauges_dir}/cbsa-{my_cbsa_sf$GEOID}.csv"))}
  
    myextent <- sp::bbox(as(my_cbsa_sf, "Spatial"))
    osm1 <- osm.raster(myextent)
    
    my_cbsa_prj <- sf::st_transform(my_cbsa_sf, 3857)
    good_sites_prj <- sf::st_transform(good_sites_sf, 3857)
    
    good_sites_prj$lat <- st_coordinates(good_sites_prj) %>% 
      as.data.frame() %>% pull(Y)
    good_sites_prj$lon <- st_coordinates(good_sites_prj) %>% 
      as.data.frame() %>% pull(X)

    m1 <- ggRGB(osm1, ggObj = TRUE, r = 1, g = 2, b = 3)
    m2 <- m1 +
      geom_sf(data = my_cbsa_prj, 
              color = "purple",
              fill = "purple", 
              alpha = 0.05) +
      # geom_sf(data = flowlines_list_sf,
      #         color = "dodgerblue") +
      geom_sf(data = good_sites_sf) +
      ggrepel::geom_text_repel(
        data = good_sites_prj,
        aes(x = lon, y = lat, label = site_no),
        nudge_y = 0.1
      ) +
      theme_minimal() +
      ggtitle(glue::glue("{my_cbsa_sf$NAME}")) +
      xlab(element_blank()) +
      ylab(element_blank())
   
    data_dir <- "/nfs/khondula-data/projects/river-cities/data"
      mapfile <- glue::glue("{data_dir}/maps/gauges_{my_cbsa_sf$NAME}.pdf")
      pdf(mapfile, height = 10, width = 8)
       print(m2)
      dev.off()
    }
  }
}



```

```{r, include = FALSE, eval=FALSE}
get_usgs_sites(cbsa_sf$GEOID[62])
purrr::walk(cbsa_sf$GEOID[63:70], ~get_usgs_sites(.x))
```

Run on cluster

```{r, eval = FALSE}
library(rslurm)
pars <- data.frame(my_cbsa = cbsa_sf$GEOID[71:938],
                   stringsAsFactors = FALSE)


sjob <- slurm_apply(get_usgs_sites, 
                    pars, 
                    jobname = 'rivercities',
                    # slurm_options = list(partition = "sesync"),
                    nodes = 20, 
                    cpus_per_node = 4,
                    submit = TRUE)

rslurm::print_job_status(sjob)
rslurm::cancel_slurm(sjob)
```

```{r, eval=FALSE}
length(list.files(gauges_dir))
```

Combine 

```{r}
# read in all csvs and save as one table
df <- fs::dir_ls("data") %>% 
  purrr::map_df(~read_csv(.x, col_types = c("cccddccccDDdllcdcd")))

df %>% readr::write_csv("river-cities.csv")

```
